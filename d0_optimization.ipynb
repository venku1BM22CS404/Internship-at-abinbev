{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta, date\n",
    "import xlrd\n",
    "import glob\n",
    "import warnings\n",
    "from pulp import *\n",
    "import pytz\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_datetime(series):\n",
    "    \"\"\"\n",
    "    Ensure that the column is a datetime object\n",
    "    \"\"\"\n",
    "    if not pd.api.types.is_datetime64_any_dtype(series):\n",
    "        series= pd.to_datetime(series, errors='coerce')\n",
    "    return series\n",
    "def ensure_level_of_data(df):\n",
    "    \"\"\"\n",
    "    Ensure that there is no duplication after pre-processing\n",
    "    \"\"\"\n",
    "    if df.duplicated().any():\n",
    "        print(\"DataFrame contains duplicates. Dropping duplicates.\")\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the location variables from the file\n",
    "with open('location_variables.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        exec(line)\n",
    "\n",
    "run_date = pd.to_datetime(run_date)\n",
    "run_time_naive = pd.to_datetime(run_time_naive)\n",
    "run_time = pd.to_datetime(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the preprocessed files\n",
    "\n",
    "stock =  pd.read_csv(preprocessed_path + 'stock.csv' )\n",
    "stock = ensure_level_of_data(stock)\n",
    "load_details_df =  pd.read_csv(preprocessed_path + 'load_details.csv' )\n",
    "load_details_df = ensure_level_of_data(load_details_df)\n",
    "obs_df =  pd.read_csv(preprocessed_path + 'obs_stock.csv' )\n",
    "obs_df = ensure_level_of_data(obs_df)\n",
    "open_sto = pd.read_csv(preprocessed_path + 'open_sto.csv' )\n",
    "open_sto = ensure_level_of_data(open_sto)\n",
    "inventory_policy = pd.read_csv(preprocessed_path + 'inventory_policy.csv' )\n",
    "inventory_policy = ensure_level_of_data(inventory_policy)\n",
    "open_so = pd.read_csv(preprocessed_path + 'open_so.csv' )\n",
    "open_so = ensure_level_of_data(open_so)\n",
    "uom_df = pd.read_csv(preprocessed_path + 'uom.csv' )\n",
    "uom_df = ensure_level_of_data(uom_df)\n",
    "production = pd.read_csv(preprocessed_path + 'planned_production.csv' )\n",
    "production = ensure_level_of_data(production)\n",
    "actual_production = pd.read_csv(preprocessed_path + 'actual_production.csv' )\n",
    "actual_production = ensure_level_of_data(actual_production)\n",
    "planned_loads_df = pd.read_csv(preprocessed_path + 'planned_loads.csv' )\n",
    "planned_loads_df = ensure_level_of_data(planned_loads_df)\n",
    "outbound_loads_df = pd.read_csv(preprocessed_path + 'outbound_loads.csv' )\n",
    "outbound_loads_df = ensure_level_of_data(outbound_loads_df)\n",
    "inbound_loads_df = pd.read_csv(preprocessed_path + 'inbound_loads.csv' )\n",
    "inbound_loads_df = ensure_level_of_data(inbound_loads_df)\n",
    "lcp_data = pd.read_csv(preprocessed_path + 'lcp_data.csv' )\n",
    "lcp_data = ensure_level_of_data(lcp_data)\n",
    "standard_weights = pd.read_csv(preprocessed_path + 'standard_weights.csv' )\n",
    "standard_weights = ensure_level_of_data(standard_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creatign dictionaries for all units of conversion\n",
    "\n",
    "pc_to_hl_dict = dict(zip(uom_df['material_sk'], uom_df['PC_HL']))\n",
    "pc_to_pal_dict = dict(zip(uom_df['material_sk'], uom_df['PC_PAL']))\n",
    "pal_weight_dict = dict(zip(uom_df['material_sk'], uom_df['pal_weight_kg']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_convert_to_dt = ['origin_slot_arrival','origin_slot_departure']\n",
    "outbound_loads_df[cols_to_convert_to_dt] = outbound_loads_df[cols_to_convert_to_dt].apply(ensure_datetime)\n",
    "cols_to_convert_to_dt = ['destination_slot_arrival','destination_slot_departure']\n",
    "inbound_loads_df[cols_to_convert_to_dt] = inbound_loads_df[cols_to_convert_to_dt].apply(ensure_datetime)\n",
    "cols_to_convert_to_dt = ['origin_slot_arrival','origin_slot_departure','destination_slot_arrival','destination_slot_departure']\n",
    "load_details_df[cols_to_convert_to_dt] = load_details_df[cols_to_convert_to_dt].apply(ensure_datetime)\n",
    "cols_to_convert_to_dt = ['start_inflow_ts','end_outflow_ts']\n",
    "production[cols_to_convert_to_dt] = production[cols_to_convert_to_dt].apply(ensure_datetime)\n",
    "\n",
    "outbound_loads_df.rename(columns={'origin_slot_arrival':'Slot Booked From', 'origin_slot_departure':'Slot Booked To'}, inplace=True)\n",
    "inbound_loads_df.rename(columns={'destination_slot_arrival':'Slot Booked From', 'destination_slot_departure':'Slot Booked To'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stock_available_sr(load, stock, open_so, open_sto_in, open_sto_out, production, actual_production, inventory_policy, run_time, run_type):\n",
    "    sku = load['material_sk']\n",
    "    source = load['Source']\n",
    "    slot_booking_time = load['Slot Booked From']\n",
    "    priority_flag = load['Priority Flag']\n",
    "\n",
    "    try:\n",
    "        # Total Stock at hand for this SKU and source location, also depends on the refersh rate\n",
    "        stock_at_hand = stock.loc[\n",
    "            (stock['material_sk'] == sku) & (stock['Source'] == source), 'Opening_Stock'].values[0]\n",
    "    \n",
    "    except IndexError:\n",
    "        stock_at_hand = 0\n",
    "\n",
    "\n",
    "    # Outgoing Open SO - total outgoing quantity of this SKU from this source for run date\n",
    "    outgoing_so = open_so[\n",
    "        (open_so['material_sk'] == sku) &\n",
    "        (open_so['Source'] == source) &\n",
    "        (open_so['Delivery Date'] == run_time.normalize())\n",
    "    ]['open_so_out(HL)'].sum()\n",
    "\n",
    "    # Incoming Open STO - total incoming quantity of this SKU to this source between model run time and (1 hour before slot booking time or truck arrival time)\n",
    "    incoming_sto = open_sto_in[\n",
    "        (open_sto_in['material_sk'] == sku) &\n",
    "        (open_sto_in['Destination'] == source) &\n",
    "        (open_sto_in['Slot Booked From'] >= run_time) &\n",
    "        (open_sto_in['Slot Booked From'] <= slot_booking_time - timedelta(hours=1))\n",
    "    ]['Total Quantity HL'].sum()\n",
    "\n",
    "    # Planned production only for now, might have to add actual production\n",
    "    # total planned production of this SKU at this source which started after (3 hours before truck arrival time or slot booking time) and supposed to end after model run time\n",
    "    planned_production = production[\n",
    "        (production['material_sk'] == sku) &\n",
    "        (production['plant_code'] == source) &\n",
    "        (production['end_outflow_ts'] >= run_time) &\n",
    "        (production['start_inflow_ts'] >= slot_booking_time - timedelta(hours=3))\n",
    "    ]['Production_HL'].sum()\n",
    "\n",
    "    try:\n",
    "        # Actual production for the whole day for this SKU and destination -- destination var is not defined always leading to error\n",
    "        #check with Rachana on this\n",
    "        actual_prod = actual_production[\n",
    "            (actual_production['material_sk'] == sku) &\n",
    "            (actual_production['plant_code'] == destination)\n",
    "        ]['Production_HL'].sum()\n",
    "    except:\n",
    "        actual_prod = 0\n",
    "\n",
    "    # Outgoing STO with priority flag check, will have to validate and check\n",
    "    # Outgoing STO - total outgoing quantity of this SKU from this source between model run time and slot booking time with priority flag less than or equal to the load priority flag\n",
    "    outgoing_sto = open_sto_out[\n",
    "        (open_sto_out['material_sk'] == sku) &\n",
    "        (open_sto_out['Source'] == source) &\n",
    "        (open_sto_out['Slot Booked From'] > run_time) &\n",
    "        (open_sto_out['Slot Booked From'] < slot_booking_time) &\n",
    "        (open_sto_out['Priority Flag'] <= priority_flag)\n",
    "    ]['Total Quantity HL'].sum()\n",
    "\n",
    "    try: \n",
    "        # Safety stock\n",
    "        safety_stock = inventory_policy.loc[(inventory_policy['material_sk'] == sku)&(inventory_policy['Source']==source), 'safety_stock'].values[0]\n",
    "    except IndexError:\n",
    "        safety_stock = 0\n",
    "    \n",
    "    ## adding IF statement here so that for initial assignment, there is no Safety Stock, but for top-ups there are\n",
    "    if run_type == 'INIT':\n",
    "        safety_stock = 0\n",
    "    elif run_type == 'TOP-UP':\n",
    "        safety_stock = safety_stock\n",
    "\n",
    "    # Calculate the stock available\n",
    "    stock_available = (stock_at_hand - outgoing_so + incoming_sto + planned_production + actual_prod - outgoing_sto - safety_stock)\n",
    "    \n",
    "    return stock_available, stock_at_hand, planned_production, actual_prod, outgoing_so + outgoing_sto, incoming_sto, safety_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stock_available_dest(load, stock, open_so, open_sto_in, open_sto_out, actual_production, production, inventory_policy, run_time):\n",
    "    sku = load['material_sk']\n",
    "    destination = load['Destination']\n",
    "    priority_flag = load['Priority Flag']\n",
    "\n",
    "    try:\n",
    "        # Stock at hand at the destination\n",
    "        stock_at_hand = stock.loc[\n",
    "            (stock['material_sk'] == sku) & (stock['Source'] == destination), 'Opening_Stock'].values[0]\n",
    "    except IndexError:\n",
    "        stock_at_hand = 0\n",
    "\n",
    "    # Outgoing SO orders for the whole day\n",
    "    outgoing_so = open_so[\n",
    "        (open_so['material_sk'] == sku) &\n",
    "        (open_so['Source'] == destination) &\n",
    "        (open_so['Delivery Date'] == run_time.normalize())\n",
    "    ]['open_so_out(HL)'].sum()\n",
    "\n",
    "    # Incoming Open STO orders for the whole day\n",
    "    incoming_sto = open_sto_in[\n",
    "        (open_sto_in['material_sk'] == sku) &\n",
    "        (open_sto_in['Destination'] == destination) &\n",
    "        (open_sto_in['Slot Booked From'].dt.normalize() == run_time.normalize())\n",
    "    ]['Total Quantity HL'].sum()\n",
    "\n",
    "    # Outgoing STO orders for the whole day\n",
    "    outgoing_sto = open_sto_out[\n",
    "        (open_sto_out['material_sk'] == sku) &\n",
    "        (open_sto_out['Source'] == destination) &\n",
    "        (open_sto_out['Slot Booked From'].dt.normalize() == run_time.normalize())\n",
    "    ]['Total Quantity HL'].sum()\n",
    "\n",
    "    try:\n",
    "        # Planned production for the whole day\n",
    "        planned_production = production[\n",
    "            (production['material_sk'] == sku) &\n",
    "            (production['plant_code'] == destination) &\n",
    "            (production['end_outflow_ts'].dt.normalize() == run_time.normalize())\n",
    "        ]['Production_HL'].sum()\n",
    "    except:\n",
    "        planned_production = 0\n",
    "\n",
    "    try:\n",
    "        # Actual production for the whole day\n",
    "        actual_prod = actual_production[\n",
    "            (actual_production['material_sk'] == sku) &\n",
    "            (actual_production['plant_code'] == destination)\n",
    "        ]['Production_HL'].sum()\n",
    "    except:\n",
    "        actual_prod = 0\n",
    "\n",
    "    # Calculate the stock available at the destination\n",
    "    stock_available = (stock_at_hand - outgoing_so + incoming_sto + planned_production + actual_prod - outgoing_sto)\n",
    "\n",
    "    try:\n",
    "        # Maximum stock (inventory policy)\n",
    "        max_stock = inventory_policy.loc[\n",
    "            (inventory_policy['material_sk'] == sku) & \n",
    "            (inventory_policy['Source'] == destination), \n",
    "            'max_stock'\n",
    "        ].values[0]\n",
    "    except IndexError:\n",
    "        max_stock = 0\n",
    "\n",
    "    # Calculate the demand at the destination\n",
    "    demand = max_stock - stock_available\n",
    "\n",
    "    try:\n",
    "        # safety stock (inventory policy)\n",
    "        safety_stock = inventory_policy.loc[\n",
    "            (inventory_policy['material_sk'] == sku) & \n",
    "            (inventory_policy['Source'] == destination), \n",
    "            'safety_stock'\n",
    "        ].values[0]\n",
    "    except IndexError:\n",
    "        safety_stock = 0\n",
    "\n",
    "    oos_qty = (safety_stock - stock_available) if (safety_stock - stock_available>=0) else 0\n",
    "    oos_per = oos_qty / demand\n",
    "    oos_per = oos_per if oos_per>0 else 0\n",
    "\n",
    "    return stock_available, demand, oos_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_end_of_day_stock(stock, open_so, open_sto_out, production, actual_production, swaps_df, run_time):\n",
    "    updated_stock = stock.copy()\n",
    "\n",
    "    for idx, row in updated_stock.iterrows():\n",
    "        sku = row['material_sk']\n",
    "        source = row['Source']\n",
    "\n",
    "        # Stock at hand\n",
    "        stock_at_hand = row['Opening_Stock']\n",
    "\n",
    "        # Outgoing SO for the whole day\n",
    "        outgoing_so = open_so[\n",
    "            (open_so['material_sk'] == sku) &\n",
    "            (open_so['Source'] == source) &\n",
    "            (open_so['Delivery Date'] == run_time.normalize())\n",
    "        ]['open_so_out(HL)'].sum()\n",
    "\n",
    "        # Updated incoming Open STO\n",
    "        incoming_sto = swaps_df[\n",
    "            (swaps_df['material_sk'] == sku) &\n",
    "            (swaps_df['Destination'] == source) &\n",
    "            (swaps_df['Suggested_deployment(HL)'] >0)\n",
    "        ]['Suggested_deployment(HL)'].sum()\n",
    "\n",
    "        incoming_og_sto = open_sto_out[\n",
    "            (open_sto_out['material_sk'] == sku) &\n",
    "            (open_sto_out['Destination']==source)&\n",
    "            (open_sto_out['at_risk_flag']==False)\n",
    "        ]['Total Quantity HL'].sum()\n",
    "\n",
    "        # Updated outgoing Open STO\n",
    "        outgoing_sto = swaps_df[\n",
    "            (swaps_df['material_sk'] == sku) &\n",
    "            (swaps_df['Source'] == source) &\n",
    "            (swaps_df['Suggested_deployment(HL)'] >0)\n",
    "        ]['Suggested_deployment(HL)'].sum()\n",
    "\n",
    "        outgoing_og_sto = open_sto_out[\n",
    "            (open_sto_out['material_sk'] == sku) &\n",
    "            (open_sto_out['Source']==source)&\n",
    "            (open_sto_out['at_risk_flag']==False)\n",
    "        ]['Total Quantity HL'].sum()\n",
    "\n",
    "        \n",
    "        try:\n",
    "            # Planned production for the whole day\n",
    "            planned_production = production[\n",
    "                (production['material_sk'] == sku) &\n",
    "                (production['plant_code'] == source) &\n",
    "                (production['Prod End'].dt.normalize() == run_time.normalize())\n",
    "            ]['Production_HL'].sum()\n",
    "        except:\n",
    "            planned_production = 0\n",
    "\n",
    "        # Calculate the stock at hand at the end of the day\n",
    "        stock_at_hand_end_of_day = stock_at_hand - outgoing_so + incoming_sto + incoming_og_sto - outgoing_sto - outgoing_og_sto + planned_production\n",
    "\n",
    "        # Update the stock dataframe\n",
    "        updated_stock.at[idx, 'Closing_Stock'] = stock_at_hand_end_of_day\n",
    "\n",
    "    return updated_stock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_loads(data):\n",
    "    problem = LpProblem('Load Exchanging', LpMaximize)\n",
    "\n",
    "    # Decision Variable for Quantity\n",
    "    data['Qty_LPVar_Name'] = 'qty_' + data['material_sk'].astype(str) + '_' + data['load_id'].astype(str)\n",
    "    data['Qty_LPVar'] = data['Qty_LPVar_Name'].apply(lambda x : LpVariable(x, lowBound=0, cat=\"Continuous\"))\n",
    "\n",
    "    # OBJECTIVE FUNCTION\n",
    "    # shipment_value = lpSum((data['lcp_rank_1'] + (0.5 * data['%_OOS_1'] + 0.5 * data['%_At_Risk_1'])/100 + data['Priority Flag_1'] + (1 - data['Waiting_time'])) * data['Qty_LPVar'] * data['HL_weight'])\n",
    "    shipment_value = lpSum((data['lcp_rank_1'] + 1 + (1 * data['%_OOS_1'] + 1 * data['%_At_Risk_1']) + data['Priority Flag_1']) * data['Qty_LPVar'])\n",
    "    problem +=shipment_value\n",
    "\n",
    "    # CONSTRAINT: Sum of all HL quantity recommendations for a material_sk should be less than total HL stock on hand at the source\n",
    "    for grp_name, grp_df in data.groupby(['material_sk', 'Source']):\n",
    "        problem += lpSum(grp_df['Qty_LPVar']) <= grp_df['Stock_on_hand_sr(HL)'].iloc[0]\n",
    "    \n",
    "    # CONSTRAINT: Sum of all HL quantity recommendations for a material_sk should be less than Demand at the Destination\n",
    "    for grp_name, grp_df in data.groupby(['material_sk', 'Destination']):\n",
    "        problem += lpSum(grp_df['Qty_LPVar']) <= grp_df['demand_at_dt(HL)'].iloc[0]\n",
    "\n",
    "    # CONSTRAINT: Sum of all recommended weights for a load should be less than the weight left on the truck(load)\n",
    "    for grp_name, grp_df in data.groupby(['load_id']):\n",
    "        problem += lpSum(qty * conv for qty, conv in zip(grp_df['Qty_LPVar'], grp_df['HL_weight'])) <= grp_df['available_Weight'].iloc[0]\n",
    "\n",
    "    # CONSTRAINT: Sum of all recommended pallets should be less than the pallet space left on the truck\n",
    "    for grp_name, grp_df in data.groupby(['load_id']):\n",
    "        problem += lpSum(qty * conv for qty, conv in zip(grp_df['Qty_LPVar'], grp_df['HL_PAL'])) <= grp_df['available_PAL'].iloc[0]\n",
    "        \n",
    "    problem.solve(PULP_CBC_CMD(maxSeconds = 2700, threads = None, msg = 0))\n",
    "\n",
    "    print(LpStatus[problem.status])\n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_loads(main_outbound_df, main_inbound_df, main_load_details, stock, open_so, open_sto, production, actual_production, inventory_policy, lcp_data, load_details_df, run_time, result_path, tag):\n",
    "    # ### For manual runs\n",
    "    # main_outbound_df = outbound_loads_df_d0[outbound_loads_df_d0['Slot Booked From']>run_time]\n",
    "    # main_inbound_df = inbound_loads_df_d0\n",
    "    # main_load_details = load_details_df_d0[load_details_df_d0['origin_slot_arrival']>run_time].reset_index(drop=True)\n",
    "    # # stock = updated_stock.copy()\n",
    "    # run_time= run_time\n",
    "    # tag = 'D0'\n",
    "\n",
    "    # Initialize a dictionary to store the results\n",
    "    kpi_results    = {}\n",
    "    print('Optimizing for '+tag)\n",
    "\n",
    "\n",
    "    # Sorting the outbound loads as per slot booked from and priority flag\n",
    "    main_outbound_df = main_outbound_df.sort_values(['Priority Flag', 'Slot Booked From']).reset_index(drop=True)\n",
    "    print('## Total Number of Loads: ', main_outbound_df['load_id'].nunique())\n",
    "    kpi_results['Total number of loads'] = {\n",
    "        'Value': main_outbound_df['load_id'].nunique(),\n",
    "        'Percentage': 100\n",
    "    }\n",
    "\n",
    "    if main_outbound_df['load_id'].nunique() == 0:\n",
    "        print('## No loads to process')\n",
    "        swaps_df = pd.DataFrame()\n",
    "        main_load_details = pd.DataFrame()\n",
    "        if tag == 'D0':\n",
    "            return swaps_df, main_load_details, stock\n",
    "        else:\n",
    "            return swaps_df, main_load_details\n",
    "\n",
    "    # Merging the outbound loads with the Open STO loads (to get material level details)\n",
    "    open_sto_out = pd.merge(main_outbound_df, open_sto, on=['load_id', 'RFRC_NUM12', 'movement_type', 'Source', 'Destination', 'origin_shipping_location_sk', 'destination_shipping_location_sk', 'Priority Flag'], how='inner')\n",
    "    open_sto_in = pd.merge(main_inbound_df, open_sto, on=['load_id', 'Source', 'RFRC_NUM12', 'movement_type', 'Destination', 'origin_shipping_location_sk', 'destination_shipping_location_sk', 'Priority Flag'] ,how='inner')\n",
    "\n",
    "    open_sto_out[['Stock_on_hand_sr(HL)', 'Stock_sr', 'Planned_production_sr', 'Actual_production_sr', 'Outgoing_SO_STO_sr', 'Incoming_STO_sr', 'Safety_Stock_sr']] = \\\n",
    "        open_sto_out.apply(lambda row: pd.Series(calculate_stock_available_sr(row, stock, open_so, open_sto_in, open_sto_out, production, actual_production, inventory_policy, run_time, 'INIT')), axis=1)\n",
    "\n",
    "    open_sto_out['at_risk_flag'] = np.where(open_sto_out['Stock_on_hand_sr(HL)'] < open_sto_out['Total Quantity HL'], True, False)\n",
    "    open_sto_out['Stock_on_hand_sr(HL)'] = np.where(open_sto_out['Stock_on_hand_sr(HL)'] < 0, 0, open_sto_out['Stock_on_hand_sr(HL)'])\n",
    "    open_sto_out['qty_at_risk'] = np.where(open_sto_out['Total Quantity HL'] - open_sto_out['Stock_on_hand_sr(HL)'] <= 0, 0, open_sto_out['Total Quantity HL'] - open_sto_out['Stock_on_hand_sr(HL)'])\n",
    "    open_sto_out['Total_feasible_order_qty'] = np.where(open_sto_out['at_risk_flag'] == True, open_sto_out['Total Quantity HL'] - open_sto_out['qty_at_risk'], open_sto_out['Total Quantity HL'])\n",
    "    print('## Number of Loads that are risk due to insufficient stock: ', open_sto_out[open_sto_out['at_risk_flag'] == True]['load_id'].nunique())\n",
    "    kpi_results['Number of Loads that are risk due to insufficient stock'] = {\n",
    "        'Value': open_sto_out[open_sto_out['at_risk_flag'] == True]['load_id'].nunique(),\n",
    "        'Percentage': open_sto_out[open_sto_out['at_risk_flag'] == True]['load_id'].nunique()/main_outbound_df['load_id'].nunique() * 100\n",
    "    }\n",
    "\n",
    "\n",
    "    # Getting UOM for weights and for HL to PAL conversion\n",
    "    open_sto_out['Total_feasible_order_qty_PC'] = open_sto_out['Total_feasible_order_qty'] / open_sto_out['material_sk'].map(pc_to_hl_dict)\n",
    "    open_sto_out['Total_feasible_order_qty_PAL'] = open_sto_out['Total_feasible_order_qty_PC'] * open_sto_out['material_sk'].map(pc_to_pal_dict)\n",
    "    open_sto_out['Total_feasible_order_qty_Weight'] = open_sto_out['Total_feasible_order_qty_PAL'] * open_sto_out['material_sk'].map(pal_weight_dict)\n",
    "\n",
    "    ### Getting load level details of the original SKUs \n",
    "    new_load_data = open_sto_out.groupby(['RFRC_NUM12', 'load_id', 'movement_type', 'Priority Flag', 'Source', 'Destination', 'origin_shipping_location_sk',\n",
    "        'destination_shipping_location_sk', 'Slot Booked From', 'Slot Booked To'], as_index=False).agg({'Total_feasible_order_qty_PAL': 'sum', 'Total_feasible_order_qty_Weight': 'sum'})\n",
    "\n",
    "    ### Combining with the load level details\n",
    "    main_load_details =pd.merge(main_load_details, new_load_data[['RFRC_NUM12', 'load_id', 'movement_type', 'Priority Flag', 'Source', 'Destination', 'Total_feasible_order_qty_PAL', 'Total_feasible_order_qty_Weight']],\\\n",
    "            on = ['RFRC_NUM12', 'load_id', 'movement_type', 'Priority Flag', 'Source', 'Destination'], how = 'left')\n",
    "\n",
    "    # Combining the main_load_details with standard pallet and weight limit\n",
    "    #Ask rachana what 26 represents here\n",
    "    main_load_details['available_PAL'] = (26 - main_load_details['Total_feasible_order_qty_PAL']).astype(int)\n",
    "    main_load_details['available_Weight'] = standard_weights.loc[standard_weights['Country'] == 'GB', 'weight_limit'].values[0] - (main_load_details['Total_feasible_order_qty_Weight']/1000)\n",
    "\n",
    "    ### Getting the labels for each load:\n",
    "    # Define a tolerance level for floating-point comparison\n",
    "    tolerance = 1e-5\n",
    "\n",
    "    # Initialize the 'Action' column with default empty strings\n",
    "    main_load_details['Action'] = ''\n",
    "\n",
    "    # Apply 'Load not at risk' condition\n",
    "    #potential bug due to the abs condition which classifies loads with order req met as blank instead of 'Load not at risk', confirm with Rachana\n",
    "    main_load_details.loc[\n",
    "        (main_load_details['Original_Quantity_Ordered_PAL'] - main_load_details['Total_feasible_order_qty_PAL']).abs() < tolerance, \n",
    "        'Action'] = 'Load not at risk'\n",
    "\n",
    "    # Apply 'At risk' condition\n",
    "    main_load_details.loc[\n",
    "        (main_load_details['Total_feasible_order_qty_PAL'] < main_load_details['Original_Quantity_Ordered_PAL']) & \n",
    "        (main_load_details['Action'] == ''),\n",
    "        'Action'] = 'At risk'\n",
    "\n",
    "    # Apply 'Light load' condition only if the other two are not satisfied\n",
    "    main_load_details.loc[\n",
    "        (main_load_details['available_PAL'] > 0) & \n",
    "        (main_load_details['available_Weight'] > 0) & \n",
    "        (main_load_details['Action'] == 'Load not at risk'), \n",
    "        'Action'] = 'Light load'\n",
    "\n",
    "    ### new_load_data => only the light loads or the ones at risk\n",
    "    new_load_data = main_load_details[main_load_details['Action'].isin(['Light load', 'At risk'])]\n",
    "    new_load_data = new_load_data[(new_load_data['available_PAL']>0)&(new_load_data['available_Weight']>0)]\n",
    "    print('## Loads that are at risk or have a light load: ', new_load_data['load_id'].nunique())\n",
    "    kpi_results['Loads that are at risk or have a light load'] = {\n",
    "        'Value': new_load_data['load_id'].nunique(),\n",
    "        'Percentage': new_load_data['load_id'].nunique()/main_outbound_df['load_id'].nunique() * 100\n",
    "    }\n",
    "\n",
    "    print('## Loads that are a light load: ', new_load_data[new_load_data['Action']=='Light load']['load_id'].nunique())\n",
    "    kpi_results['Loads that are a light load'] = {\n",
    "        'Value': new_load_data[new_load_data['Action']=='Light load']['load_id'].nunique(),\n",
    "        'Percentage': new_load_data[new_load_data['Action']=='Light load']['load_id'].nunique()/main_outbound_df['load_id'].nunique() * 100\n",
    "    }\n",
    "\n",
    "    if new_load_data['load_id'].nunique()==0:\n",
    "        print('## No Loads at Risk')\n",
    "        swaps_df = pd.DataFrame()\n",
    "        main_load_details = pd.DataFrame()\n",
    "        if tag == 'D0':\n",
    "            return swaps_df, main_load_details, stock\n",
    "        else:\n",
    "            return swaps_df, main_load_details\n",
    "\n",
    "    ## remove since this has been added to preprocessing\n",
    "    # lcp_data.rename(columns={'origin_location_code':'Source', 'destination_location_code':'Destination', 'origin_plant_sk':'origin_shipping_location_sk', 'destination_plant_sk':'destination_shipping_location_sk'}, inplace=True)\n",
    "    # lcp_data.drop(columns=['origin_shipping_location_sk', 'destination_shipping_location_sk'], inplace=True)\n",
    "\n",
    "\n",
    "    ### Gotta add the origin and the destination sk\n",
    "    data = pd.merge(new_load_data, lcp_data, on=['Source', 'Destination'], how='inner')\n",
    "    data.rename(columns={'origin_slot_arrival':'Slot Booked From', 'origin_slot_departure':'Slot Booked To'}, inplace=True)\n",
    "\n",
    "\n",
    "    data[['Stock_on_hand_sr(HL)', 'Stock_sr', 'Planned_production_sr', 'Actual_production_sr', 'Outgoing_SO_STO_sr', 'Incoming_STO_sr', 'Safety_Stock_sr']] = \\\n",
    "        data.apply(lambda row: pd.Series(calculate_stock_available_sr(row, stock, open_so, open_sto_in, open_sto_out, production, actual_production, inventory_policy, run_time, 'TOP-UP')), axis=1)\n",
    "    data = data[data['Stock_on_hand_sr(HL)'] > 0]\n",
    "\n",
    "    data[['Stock_on_hand_dt(HL)', 'demand_at_dt(HL)', '%_OOS']] = data.apply(lambda row: pd.Series(calculate_stock_available_dest(row, stock, open_so, open_sto_in, open_sto_out, production, actual_production, inventory_policy, run_time)), axis=1)\n",
    "    data = data[(data['demand_at_dt(HL)'] > 0.0)&(data['Stock_on_hand_sr(HL)'] > 0.0)]\n",
    "\n",
    "\n",
    "    print('## Loads that can have stock switched out using LCP data: ', data['load_id'].nunique())\n",
    "    kpi_results['Loads that can have stock switched out using LCP data'] = {\n",
    "        'Value':  data['load_id'].nunique(),\n",
    "        'Percentage':  data['load_id'].nunique()/main_outbound_df['load_id'].nunique() * 100\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    if data.shape[0]==0:\n",
    "        print('## No possible replacement for the loads at risk')\n",
    "        data.to_excel(f\"{result_path}{tag}_pre_opti_model.xlsx\", index=False)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    data = data[['load_id', 'RFRC_NUM12', 'movement_type', 'Source', 'Destination', 'Slot Booked From',\n",
    "                    'Slot Booked To', 'Priority Flag', 'available_PAL', 'available_Weight',\n",
    "                    'material_sk', 'material_code', 'lcp_rank', 'Stock_on_hand_sr(HL)', 'Stock_sr', 'Planned_production_sr', 'Actual_production_sr', 'Outgoing_SO_STO_sr', 'Incoming_STO_sr', 'Safety_Stock_sr', 'demand_at_dt(HL)', '%_OOS', 'Action']]\n",
    "    data['HL_PAL'] = data['material_sk'].map(pc_to_pal_dict) / data['material_sk'].map(pc_to_hl_dict)\n",
    "    data['HL_weight'] = data['material_sk'].map(pal_weight_dict) * data['HL_PAL'] / 1000\n",
    "\n",
    "    data = pd.merge(data, obs_df[['material_code', 'Source', 'material_sk', '%_At_Risk']], on=['material_sk', 'material_code', 'Source'], how='left').fillna(0)\n",
    "\n",
    "    data.drop_duplicates(subset=['load_id', 'RFRC_NUM12', 'movement_type', 'Source', 'Destination', 'Slot Booked From',\n",
    "        'Slot Booked To', 'Priority Flag', 'material_sk', 'material_code'], inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "    # Operations to calculate the weights\n",
    "    data['Waiting_time'] = data['Slot Booked From'] - run_time\n",
    "    data['Waiting_time'] = data['Waiting_time'] / data['Waiting_time'].max()\n",
    "\n",
    "    #### Change this: priority Flag?\n",
    "    # Create a dictionary with the formula (15 - x) * 3 + 1 for Priority Flag values 0 to 16\n",
    "    priority_mapping = {x: (15 - x) * 3 + 1 for x in range(17)}\n",
    "    # Apply the dictionary to map the 'Priority Flag' column\n",
    "    data['Priority Flag_1'] = data['Priority Flag'].map(priority_mapping)\n",
    "    #bug here, %OOS returned is always <1, not multiplied by 100 in the function\n",
    "    data['%_OOS_1'] = np.where(data['%_OOS'] >= 200, 200, data['%_OOS'])\n",
    "    data['%_At_Risk_1'] = np.where(data['%_At_Risk'] >= 100, 100, data['%_At_Risk'])\n",
    "    # Define the dictionary for mapping lcp_rank values\n",
    "    lcp_rank_mapping = {\n",
    "        1: 5,\n",
    "        2: 4,  # You can assign 2 to other values if needed\n",
    "        3: 3,\n",
    "        4: 2,  # Example mapping for the remaining values\n",
    "        5: 1\n",
    "    }\n",
    "\n",
    "    # Apply the dictionary to map the 'lcp_rank' column\n",
    "    data['lcp_rank_1'] = data['lcp_rank'].map(lcp_rank_mapping)\n",
    "\n",
    "\n",
    "    data['Weights'] = ((data['lcp_rank_1']) + (1 * data['%_OOS_1'] + 1 * data['%_At_Risk_1']) + (data['Priority Flag_1']))\n",
    "\n",
    "    data.to_excel(f\"{result_path}{tag}_pre_opti_model.xlsx\", index=False)\n",
    "    temp = new_load_data[new_load_data['load_id'].isin(data['load_id'].unique())]\n",
    "\n",
    "    # Model run here\n",
    "    problem = optimise_loads(data)\n",
    "\n",
    "    # POST PROCESSING \n",
    "\n",
    "    data['Suggested_deployment(HL)'] = data['Qty_LPVar'].apply(lambda x: x.value())\n",
    "    data['LP_Result_Status'] = LpStatus[problem.status]\n",
    "    # data['Suggested_deployment(PAL)'] = data['Suggested_deployment(HL)'] * data['HL_PAL']\n",
    "    # data['Suggested_deployment(Weight)'] = data['Suggested_deployment(HL)'] * data['HL_weight']\n",
    "    results = data[data['Suggested_deployment(HL)'] > 0]\n",
    "    results = results[['load_id', 'RFRC_NUM12', 'movement_type', 'Source', 'Destination', 'Priority Flag', \\\n",
    "        'material_sk', 'material_code', 'Stock_on_hand_sr(HL)', 'Stock_sr', 'Planned_production_sr', 'Actual_production_sr', 'Outgoing_SO_STO_sr', 'Incoming_STO_sr', 'Safety_Stock_sr'\n",
    "        , 'Action', 'Suggested_deployment(HL)', 'demand_at_dt(HL)', '%_OOS']]\n",
    "    \n",
    "\n",
    "    # Swaps file creation\n",
    "    swaps_df = open_sto_out[['load_id', 'RFRC_NUM12', 'movement_type', 'sales_document_item_code', 'Source', 'Destination', 'Priority Flag', 'material_sk', 'material_code',\n",
    "                            'Stock_on_hand_sr(HL)', 'Stock_sr', 'Planned_production_sr', 'Actual_production_sr', 'Outgoing_SO_STO_sr', 'Incoming_STO_sr', 'Safety_Stock_sr', 'qty_at_risk', 'Total_feasible_order_qty', 'at_risk_flag']]\n",
    "    swaps_df = swaps_df[swaps_df['at_risk_flag'] == True]\n",
    "\n",
    "    swaps_df['Action'] = 'Swap-Out'\n",
    "    swaps_df = swaps_df.rename(columns={'qty_at_risk': 'Swap_out_qty_HL', 'Total_feasible_order_qty': 'Suggested_deployment(HL)'})\n",
    "\n",
    "    swaps_df = pd.concat([swaps_df, results])\n",
    "\n",
    "\n",
    "    swaps_df['HL_PAL'] = swaps_df['material_sk'].map(pc_to_pal_dict) / swaps_df['material_sk'].map(pc_to_hl_dict)\n",
    "    swaps_df['HL_weight'] = swaps_df['material_sk'].map(pal_weight_dict) * swaps_df['HL_PAL'] / 1000\n",
    "    swaps_df['Suggested_deployment(PAL)'] = swaps_df['Suggested_deployment(HL)'] * swaps_df['HL_PAL']\n",
    "    swaps_df['Suggested_deployment(Weight)'] = swaps_df['Suggested_deployment(HL)'] * swaps_df['HL_weight']\n",
    "    swaps_df['Suggested_deployment(PC)'] = swaps_df['Suggested_deployment(PAL)'] / swaps_df['material_sk'].map(pc_to_pal_dict)\n",
    "    swaps_df['Rounded_Suggested_deployment(PAL)'] = np.round(swaps_df['Suggested_deployment(PAL)'])\n",
    "    swaps_df['Rounded_Suggested_deployment(HL)'] = swaps_df['Rounded_Suggested_deployment(PAL)'] /swaps_df['HL_PAL'] *1000\n",
    "    swaps_df['Rounded_Suggested_deployment(Weight)'] = swaps_df['Rounded_Suggested_deployment(HL)'] * swaps_df['HL_weight']\n",
    "    swaps_df['Rounded_Suggested_deployment(PC)'] = swaps_df['Rounded_Suggested_deployment(PAL)'] / swaps_df['material_sk'].map(pc_to_pal_dict)\n",
    "\n",
    "    swapped_load_details = swaps_df.groupby(['load_id'], as_index = False).agg({'Rounded_Suggested_deployment(HL)':'sum', 'Rounded_Suggested_deployment(PAL)':'sum', 'Rounded_Suggested_deployment(Weight)':'sum'})\n",
    "    main_load_details = pd.merge(main_load_details, swapped_load_details[['load_id', 'Rounded_Suggested_deployment(HL)', 'Rounded_Suggested_deployment(PAL)', 'Rounded_Suggested_deployment(Weight)']], on='load_id', how='left')\n",
    "    main_load_details[['Rounded_Suggested_deployment(HL)', 'Rounded_Suggested_deployment(PAL)', 'Rounded_Suggested_deployment(Weight)']] = main_load_details[['Rounded_Suggested_deployment(HL)', 'Rounded_Suggested_deployment(PAL)', 'Rounded_Suggested_deployment(Weight)']].fillna(0)\n",
    "    #bug : for overloaded trucks, total pal (26) - feasible order PAL is negative, need to check with Rachana\n",
    "    main_load_details['Cancel_load'] = np.where((26-main_load_details['Total_feasible_order_qty_PAL']-main_load_details['Rounded_Suggested_deployment(PAL)'])/26 > 0.8, 'Yes', 'No')\n",
    "    main_load_details['Rounded_Suggested_deployment(Weight)'] = main_load_details['Rounded_Suggested_deployment(Weight)'] / 1000\n",
    "\n",
    "    main_load_details['Agreement to Recommendation(Yes/No)']= ''\n",
    "    main_load_details['Recommendation Executed(Yes/No)']= ''\n",
    "    main_load_details['Reason for non-agreement/non-execution']= ''\n",
    "\n",
    "    main_load_details.drop_duplicates(subset=['load_id'], inplace=True)\n",
    "\n",
    "    main_load_details = main_load_details[['RFRC_NUM12', 'load_id', 'movement_type', 'Priority Flag', 'Source',\n",
    "        'Destination', 'origin_shipping_location_sk',\n",
    "        'destination_shipping_location_sk', 'origin_slot_arrival',\n",
    "        'origin_slot_departure', 'destination_slot_arrival',\n",
    "        'destination_slot_departure', 'Action', 'Original_Quantity_Ordered_HL',\n",
    "        'Original_Quantity_Ordered_PAL', 'Original_Quantity_Ordered_KG',\n",
    "        'Total_feasible_order_qty_PAL', 'Total_feasible_order_qty_Weight',\n",
    "        'available_PAL', 'available_Weight', \n",
    "        'Rounded_Suggested_deployment(HL)', 'Rounded_Suggested_deployment(PAL)',\n",
    "        'Rounded_Suggested_deployment(Weight)', 'Cancel_load',\n",
    "        'Agreement to Recommendation(Yes/No)',\n",
    "        'Recommendation Executed(Yes/No)',\n",
    "        'Reason for non-agreement/non-execution']]\n",
    "\n",
    "    #bug : for overloaded trucks, total pal (26) - feasible order PAL is negative, need to check with Rachana\n",
    "    main_load_details['available_PAL'] = np.where(main_load_details['available_PAL']<0, 0, main_load_details['available_PAL'])\n",
    "    main_load_details['available_Weight'] = np.where(main_load_details['available_Weight']<0, 0, main_load_details['available_Weight'])\n",
    "\n",
    "    main_load_details['Day_tag'] = tag\n",
    "        \n",
    "    # Generating output file 1 with the at-risk loads\n",
    "    main_load_details.to_excel(f\"{result_path}{tag}_load_level_report.xlsx\", index=False)\n",
    "\n",
    "    swaps_df = pd.merge(swaps_df, main_load_details[['RFRC_NUM12', 'origin_slot_arrival', 'origin_slot_departure', 'destination_slot_arrival', 'destination_slot_departure', 'Total_feasible_order_qty_PAL', 'Total_feasible_order_qty_Weight','origin_shipping_location_sk', 'destination_shipping_location_sk', 'Cancel_load']], on = ['RFRC_NUM12'], how= 'left')\n",
    "    swaps_df['Swap_out_qty_HL'] = swaps_df['Swap_out_qty_HL'].fillna(0)\n",
    "    # swaps_df = pd.merge(swaps_df, new_load_data.groupby(['load_id'], as_index=False)[['Total_feasible_order_qty_PAL', 'Total_feasible_order_qty_Weight']].sum(), on = ['load_id'], how = 'left')\n",
    "\n",
    "    swaps_df['Action'] = np.where((swaps_df['Swap_out_qty_HL']>0)&(swaps_df['Suggested_deployment(HL)']>0), 'Swap-out (Update)', swaps_df['Action'])\n",
    "    swaps_df['Action'] = np.where((swaps_df['Swap_out_qty_HL']>0)&(swaps_df['Suggested_deployment(HL)']==0), 'Swap-out (Delete)', swaps_df['Action'])\n",
    "\n",
    "    swaps_df = pd.merge(swaps_df, open_sto_out[['load_id', 'material_sk', 'Total Quantity HL']], on = ['load_id', 'material_sk'], how = 'left').fillna(0)\n",
    "\n",
    "    swaps_df['Action'] = np.where((swaps_df['Action']=='Light load')&(swaps_df['Total Quantity HL']==0), 'Top-up (New)', swaps_df['Action'])\n",
    "    swaps_df['Action'] = np.where((swaps_df['Action']=='Light load')&(swaps_df['Total Quantity HL']!=0), 'Top-up (Update)', swaps_df['Action'])\n",
    "    swaps_df['Action'] = np.where((swaps_df['Action']=='At risk'), 'Swap-in', swaps_df['Action'])\n",
    "\n",
    "\n",
    "    swaps_df['Agreement to Recommendation(Yes/No)']= ''\n",
    "    swaps_df['Recommendation Executed(Yes/No)']= ''\n",
    "    swaps_df['Reason for non-agreement/non-execution']= ''\n",
    "\n",
    "    swaps_df = swaps_df.fillna(0)\n",
    "\n",
    "    swaps_df = swaps_df[['RFRC_NUM12', 'load_id', 'movement_type', 'sales_document_item_code', 'Priority Flag', 'origin_slot_arrival', 'origin_slot_departure', 'Source',\n",
    "        'Destination', 'origin_shipping_location_sk', 'destination_shipping_location_sk', \n",
    "        'material_sk', 'material_code', 'Action', 'Stock_on_hand_sr(HL)', 'Stock_sr', 'Planned_production_sr', 'Actual_production_sr', 'Outgoing_SO_STO_sr', 'Incoming_STO_sr', 'Safety_Stock_sr', \n",
    "        'demand_at_dt(HL)', '%_OOS', 'Total_feasible_order_qty_PAL',\n",
    "        'Total_feasible_order_qty_Weight', 'Swap_out_qty_HL', 'Suggested_deployment(HL)',\n",
    "        'Suggested_deployment(PAL)', 'Suggested_deployment(PC)',\n",
    "        'Suggested_deployment(Weight)', 'Rounded_Suggested_deployment(HL)',\n",
    "        'Rounded_Suggested_deployment(PAL)', 'Rounded_Suggested_deployment(PC)',\n",
    "        'Rounded_Suggested_deployment(Weight)', 'Cancel_load',\n",
    "        'Agreement to Recommendation(Yes/No)',\n",
    "        'Recommendation Executed(Yes/No)',\n",
    "        'Reason for non-agreement/non-execution']]\n",
    "\n",
    "    swaps_df['Day_tag'] = tag\n",
    "        \n",
    "    swaps_df.rename(columns={'RFRC_NUM12':'STO Number'}, inplace=True)\n",
    "\n",
    "    ### Dropping rows where the rounded recommendations for Swap-in or top-up are equal to zero\n",
    "    swaps_df = swaps_df[(~swaps_df['Action'].isin(['Top-up (New)', 'Top-up (Update)', 'Swap-in']))|(swaps_df['Rounded_Suggested_deployment(PAL)']!=0)]\n",
    "\n",
    "\n",
    "    # Writing the swaps file\n",
    "    swaps_df.to_excel(f\"{result_path}{tag}_Swaps.xlsx\", index=False)\n",
    "    \n",
    "    print('### Number of loads that got swap-ins and top-ups: ', swaps_df[swaps_df['Action'].isin(['Top-up (New)', 'Swap-in', 'Top-up (Update)'])]['load_id'].nunique())\n",
    "    kpi_results['Number of loads that got swap-ins and top-ups'] = {\n",
    "        'Value':  swaps_df[swaps_df['Action'].isin(['Top-up (New)', 'Swap-in', 'Top-up (Update)'])]['load_id'].nunique(),\n",
    "        'Percentage':  swaps_df[swaps_df['Action'].isin(['Top-up (New)', 'Swap-in', 'Top-up (Update)'])]['load_id'].nunique()/main_outbound_df['load_id'].nunique() * 100\n",
    "    }\n",
    "\n",
    "    print('### Number of swap-ins and top-ups: ', swaps_df[swaps_df['Action'].isin(['Top-up (New)', 'Swap-in', 'Top-up (Update)'])].shape[0])\n",
    "    kpi_results['Number of swap-ins and top-ups'] = {\n",
    "        'Value':  swaps_df[swaps_df['Action'].isin(['Top-up (New)', 'Swap-in', 'Top-up (Update)'])].shape[0],\n",
    "        'Percentage':  '-'\n",
    "    }\n",
    "\n",
    "    print('### Number of loads that got swap-ins: ', swaps_df[swaps_df['Action']=='Swap-in']['load_id'].nunique())\n",
    "    kpi_results['Number of loads that got swap-ins'] = {\n",
    "        'Value':  swaps_df[swaps_df['Action']=='Swap-in']['load_id'].nunique(),\n",
    "        'Percentage':  swaps_df[swaps_df['Action']=='Swap-in']['load_id'].nunique()/main_outbound_df['load_id'].nunique() * 100\n",
    "    }\n",
    "    print('### Number of swap-ins: ', swaps_df[swaps_df['Action']=='Swap-in'].shape[0])\n",
    "    kpi_results['Number of swap-ins'] = {\n",
    "        'Value':  swaps_df[swaps_df['Action']=='Swap-in'].shape[0],\n",
    "        'Percentage':  '-'\n",
    "    }\n",
    "\n",
    "    ### initial volume:\n",
    "    print('### Total initial Volume: ', main_load_details['Original_Quantity_Ordered_PAL'].sum())\n",
    "    kpi_results['Total initial Volume in Pallets'] = {\n",
    "        'Value':  main_load_details['Original_Quantity_Ordered_PAL'].sum(),\n",
    "        'Percentage':  main_load_details['Original_Quantity_Ordered_PAL'].sum()/main_load_details['Original_Quantity_Ordered_PAL'].sum() * 100\n",
    "    }\n",
    "    ### Volumne at risk\n",
    "    print('### Volume at risk: ', main_load_details['Original_Quantity_Ordered_PAL'].sum() - main_load_details['Total_feasible_order_qty_PAL'].sum())\n",
    "    kpi_results['Volume at risk in Pallets'] = {\n",
    "        'Value':  main_load_details['Original_Quantity_Ordered_PAL'].sum() - main_load_details['Total_feasible_order_qty_PAL'].sum(),\n",
    "        'Percentage':  (main_load_details['Original_Quantity_Ordered_PAL'].sum() - main_load_details['Total_feasible_order_qty_PAL'].sum())/main_load_details['Original_Quantity_Ordered_PAL'].sum() * 100\n",
    "    }\n",
    "    ### Volume at risk that can be optimized for\n",
    "    print('### Volume that can have stock switched in using LCP data: ', temp['Original_Quantity_Ordered_PAL'].sum()- temp['Total_feasible_order_qty_PAL'].sum())\n",
    "    kpi_results['Volume that can have stock switched in using LCP data in Pallets'] = {\n",
    "        'Value':  temp['Original_Quantity_Ordered_PAL'].sum()- temp['Total_feasible_order_qty_PAL'].sum(),\n",
    "        'Percentage':  (temp['Original_Quantity_Ordered_PAL'].sum()- temp['Total_feasible_order_qty_PAL'].sum())/main_load_details['Original_Quantity_Ordered_PAL'].sum() * 100\n",
    "    }\n",
    "    ### Total optimised volume:\n",
    "    print('### Total optimized volume: ', main_load_details['Rounded_Suggested_deployment(PAL)'].sum())\n",
    "    kpi_results['Total optimized volume in Pallets'] = {\n",
    "        'Value':  main_load_details['Rounded_Suggested_deployment(PAL)'].sum(),\n",
    "        'Percentage':  (main_load_details['Rounded_Suggested_deployment(PAL)'].sum())/main_load_details['Original_Quantity_Ordered_PAL'].sum() * 100\n",
    "    }\n",
    "    print('### Total optimized volume of swap-ins and top-ups: ', swaps_df[swaps_df['Action'].isin(['Top-up (New)', 'Swap-in', 'Top-up (Update)'])]['Rounded_Suggested_deployment(PAL)'].sum())\n",
    "    kpi_results['Total optimized volume of swap-ins and top-ups in Pallets'] = {\n",
    "        'Value':  swaps_df[swaps_df['Action'].isin(['Top-up (New)', 'Swap-in', 'Top-up (Update)'])]['Rounded_Suggested_deployment(PAL)'].sum(),\n",
    "        'Percentage':  (swaps_df[swaps_df['Action'].isin(['Top-up (New)', 'Swap-in', 'Top-up (Update)'])]['Rounded_Suggested_deployment(PAL)'].sum())/main_load_details['Original_Quantity_Ordered_PAL'].sum() * 100\n",
    "    }\n",
    "    print('### Total optimized volume of swap-ins: ', swaps_df[swaps_df['Action']=='Swap-in']['Rounded_Suggested_deployment(PAL)'].sum())\n",
    "    kpi_results['Total optimized volume of swap-ins in Pallets'] = {\n",
    "        'Value':  swaps_df[swaps_df['Action']=='Swap-in']['Rounded_Suggested_deployment(PAL)'].sum(),\n",
    "        'Percentage':  (swaps_df[swaps_df['Action']=='Swap-in']['Rounded_Suggested_deployment(PAL)'].sum())/main_load_details['Original_Quantity_Ordered_PAL'].sum() * 100\n",
    "    }\n",
    "\n",
    "    # Convert KPI results into a DataFrame for saving as a table\n",
    "    kpi_df = pd.DataFrame.from_dict(kpi_results, orient='index')\n",
    "\n",
    "    # Save to excel file (or any other format)\n",
    "    kpi_df.to_excel(f\"{result_path}{tag}_Score_card.xlsx\")\n",
    "\n",
    "    if tag =='D1':\n",
    "        return swaps_df, main_load_details\n",
    "        \n",
    "    ## Need to recheck the open_sto_in and open_sto_out part.\n",
    "    updated_stock = calculate_end_of_day_stock(stock, open_so, open_sto_out, production, actual_production, swaps_df, run_time)\n",
    "\n",
    "    return swaps_df, main_load_details, updated_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting the outbound and inbound STO loads for D0 and D+1\n",
    "outbound_loads_df_d0 = outbound_loads_df[outbound_loads_df['Slot Booked From'].between(run_time.normalize(), run_time.normalize()+ timedelta(days=1) - timedelta(seconds= 1))]\n",
    "outbound_loads_df_d1 = outbound_loads_df[outbound_loads_df['Slot Booked From'].between(run_time.normalize()+timedelta(days=1), run_time.normalize()+ timedelta(days=2))]\n",
    "\n",
    "inbound_loads_df_d0 = inbound_loads_df[inbound_loads_df['Slot Booked From'].between(run_time.normalize(), run_time.normalize()+ timedelta(days=1) - timedelta(seconds= 1))]\n",
    "inbound_loads_df_d1 = inbound_loads_df[inbound_loads_df['Slot Booked From'].between(run_time.normalize()+timedelta(days=1), run_time.normalize()+ timedelta(days=2))]\n",
    "\n",
    "load_details_df_d0 = load_details_df[load_details_df['origin_slot_arrival'].between(run_time.normalize(), run_time.normalize()+ timedelta(days=1) - timedelta(seconds= 1))]\n",
    "load_details_df_d1 = load_details_df[load_details_df['origin_slot_arrival'].between(run_time.normalize()+timedelta(days=1), run_time.normalize()+ timedelta(days=2))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running for D0\n",
    "### Combining with the stock on hand with the outbound load orders  \n",
    "## Filtering for future loads from the time of the run\n",
    "main_outbound_df = outbound_loads_df_d0[outbound_loads_df_d0['Slot Booked From']>run_time]\n",
    "main_inbound_df = inbound_loads_df_d0\n",
    "main_load_details = load_details_df_d0[load_details_df_d0['origin_slot_arrival']>run_time].reset_index(drop=True)\n",
    "\n",
    "\n",
    "swaps_df_d0, main_load_details_d0, updated_stock = process_loads(main_outbound_df, main_inbound_df, main_load_details, stock, open_so, open_sto, production, actual_production, inventory_policy, lcp_data, load_details_df, run_time, result_path, 'D0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaps_df_d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### INitla volume:\n",
    "# main_load_details_d0['Original_Quantity_Ordered_PAL'].sum()\n",
    "# ### Volumne at risk\n",
    "# main_load_details_d0['Original_Quantity_Ordered_PAL'].sum() - main_load_details_d0['Total_feasible_order_qty_PAL'].sum()\n",
    "# ### Volume at risk that can be optimized for\n",
    "\n",
    "# ### Total optimised volume:\n",
    "# main_load_details_d0['Rounded_Suggested_deployment(PAL)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running for D1:\n",
    "run_time_d1 = run_date +timedelta(days =1 )\n",
    "main_outbound_df = outbound_loads_df_d1\n",
    "main_inbound_df = inbound_loads_df_d1\n",
    "main_load_details = load_details_df_d1[load_details_df_d1['origin_slot_arrival']>run_time_d1].reset_index(drop=True)\n",
    "\n",
    "\n",
    "swaps_df_d1, main_load_details_d1 = process_loads(main_outbound_df, main_inbound_df, main_load_details, updated_stock, open_so, open_sto, production, actual_production, inventory_policy, lcp_data, load_details_df, run_time_d1, result_path, 'D1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Completed Running optimizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
